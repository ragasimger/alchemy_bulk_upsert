{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8f90fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "### SQL Alchemy Dependencies ###\n",
    "import sqlalchemy\n",
    "from sqlalchemy.orm import sessionmaker, mapper\n",
    "from sqlalchemy import create_engine, exc, text\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.inspection import inspect\n",
    "from sqlalchemy.orm.exc import UnmappedClassError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22bb2eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the db is running in background\n",
    "db_backend = 'mysql+pymysql://' #MySQL DB\n",
    "'''\n",
    "    Postgres(Psycopg2) = postgresql+psycopg2://\n",
    "    MsSql(Microsoft SQL Server) = mssql+pymssql://\n",
    "    For more db backend : https://docs.sqlalchemy.org/en/20/core/engines.html#backend-specific-urls\n",
    "'''\n",
    "db_server = 'localhost'\n",
    "db_name = 'test_db'\n",
    "db_username = 'secret_user' # Of course, the secret user\n",
    "db_password = 'public_password' # Why not public?\n",
    "db_params = f\"{db_backend}{db_username}:{db_password}@{db_server}/{db_name}\"\n",
    "engine = sqlalchemy.create_engine(db_params)\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "# session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a734486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# path = 'datas/insert_df_50k.csv'\n",
    "path = 'datas/upsert_df_100k.csv'\n",
    "csv_df = pd.read_csv(path)\n",
    "non_primary_id = 'non_primary_id'\n",
    "# non_primary_id list in file -> f_npk_list\n",
    "f_npk_list = csv_df[non_primary_id].to_list()\n",
    "\n",
    "'''\n",
    "  Case here:\n",
    "    1. There exist 2 different ids, may be varchar of one column(typecast according to your need).\n",
    "    2. We will be selecting with tuple of non pk id to extract the actual pk of table, and non pk id.\n",
    "    3. We will be playing with the DF, now.\n",
    "    4. We can segregate the rows of data of DF, one to insert, another for update.\n",
    "      i) Select id, non pk with tuple(non pk list).\n",
    "      ii) Convert the series data of non pk to list.\n",
    "      iii) Convert the series data of actual pk to list.\n",
    "      iv) Mask the DF of csv data with non pk list.\n",
    "        target_values = [int(_) for _ in db_pk_list]\n",
    "        column_name = \"non_pk\"\n",
    "        mask = csv_df[column_name].isin(target_values)\n",
    "      v) Now, segregate with the help of mask,\n",
    "        # Insert--Filtered DF ==> that doesn't exist in the database\n",
    "        insert_df = csv_df[~mask]\n",
    "        # Update--Dropped DF ==> that does exist in the database\n",
    "        update_df = csv_df[mask]\n",
    "    5. Never forget to make it more efficient.\n",
    "        need_update_df = False\n",
    "        need_insert_df = False\n",
    "        if len(insert_df.index) > 0:\n",
    "          need_insert_df = True\n",
    "        if len(update_df.index) > 0:\n",
    "          need_update_df = True\n",
    "'''\n",
    "%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04c70491",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_npk_list = tuple(f_npk_list)\n",
    "table_name = 'test_table'\n",
    "existing_data_query = text(f\"SELECT {non_primary_id}, id FROM {table_name} WHERE {non_primary_id} IN {csv_npk_list}\")\n",
    "existing_rows = pd.read_sql(sql=existing_data_query, con=engine)\n",
    "db_npk_list = existing_rows[non_primary_id].to_list()\n",
    "db_id_list = existing_rows['id'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d204b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_values = db_non_pk_list\n",
    "target_values = [int(_) for _ in db_npk_list]\n",
    "# Filter and remove rows with target values in the 'npk' column\n",
    "column_name = non_primary_id\n",
    "mask = csv_df[column_name].isin(target_values)\n",
    "# Insert--Filtered DF ==> that doesn't exist in the database\n",
    "insert_df = csv_df[~mask]\n",
    "# Update--Dropped DF ==> that does exist in the database\n",
    "update_df = csv_df[mask]\n",
    "need_insert_df = False\n",
    "need_update_df = False\n",
    "if len(insert_df) > 0:\n",
    "    need_insert_df = True\n",
    "if len(update_df) > 0:\n",
    "    need_update_df = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9446c963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\regmi\\AppData\\Local\\Temp/ipykernel_7220/240459645.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  update_df['id'] = existing_rows_id['id'].to_list()\n"
     ]
    }
   ],
   "source": [
    "# Processing the df data for the update method\n",
    "update_list_of_dicts = {}\n",
    "if need_update_df:\n",
    "    non_primary_id_list = update_df[non_primary_id].to_list()\n",
    "    update_non_primary_id_tuple = tuple(non_primary_id_list)\n",
    "    existing_boid_query = text(f\"SELECT id FROM {table_name} WHERE {non_primary_id} IN {update_non_primary_id_tuple}\")\n",
    "    existing_rows_id = pd.read_sql(sql=existing_data_query, con=engine)\n",
    "    update_df['id'] = existing_rows_id['id'].to_list()\n",
    "    # moving the pk to 0th index as the docs recommend\n",
    "    # to place id as first value to map the existing data\n",
    "    desired_index = 0\n",
    "    column_to_move = 'id'\n",
    "    column = update_df.pop(column_to_move)\n",
    "    update_df.insert(desired_index, column_to_move, column)\n",
    "    update_list_of_dicts = update_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d3552",
   "metadata": {},
   "source": [
    "\n",
    "## Mapper Function For Alchemy\n",
    "\n",
    "> Autoload the db  with engine and create a mapper to communicate with existing db\n",
    "\n",
    "> If you are working with the Temporary table, or trying to create programattically, follow the guide here: https://docs.sqlalchemy.org/en/20/core/metadata.html#specifying-the-schema-name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6dd29a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base = automap_base()\n",
    "Base.prepare(autoload_with=engine)\n",
    "# Make sure to update the table name\n",
    "# nice_table_name >> you_are_nice\n",
    "Nice_Table_Name = Base.classes.test_table\n",
    "# You_Are_Nice = Base.classes.you_are_nice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2faa3c7",
   "metadata": {},
   "source": [
    "# Let Me Insert Now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76b6124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.5638320446014404\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "# print('start_time',start_time)\n",
    "if need_insert_df:\n",
    "    session = Session()\n",
    "    from sqlalchemy import insert\n",
    "    insert_list_of_dicts = insert_df.to_dict(orient='records')\n",
    "    session.bulk_insert_mappings(\n",
    "        mapper=Nice_Table_Name,\n",
    "        mappings=insert_list_of_dicts\n",
    "    )\n",
    "    session.commit()\n",
    "    session.close()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "# print('end_time', end_time)\n",
    "print(f\"Execution time: {execution_time}\")\n",
    "# insert_df\n",
    "print(len(insert_list_of_dicts))\n",
    "# %time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47e8e7",
   "metadata": {},
   "source": [
    "# Let's be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33c551e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Wall time: 0 ns\n",
      "Execution time: 15.232576131820679\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "start_time = time.time()\n",
    "# print('start_time',start_time)\n",
    "if need_update_df:\n",
    "    session = Session()\n",
    "    from sqlalchemy import update\n",
    "    session.bulk_update_mappings(\n",
    "        mapper=Nice_Table_Name,\n",
    "        mappings=update_list_of_dicts\n",
    "    )\n",
    "    session.commit()\n",
    "    session.close()\n",
    "print(len(update_list_of_dicts))\n",
    "%time\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "# print('end_time', end_time)\n",
    "print(f\"Execution time: {execution_time}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
